{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Building a Basic RAG System\n",
    "\n",
    "In the previous demo, we saw LLMs fail due to:\n",
    "- Knowledge cutoff\n",
    "- Hallucinations\n",
    "- No access to private data\n",
    "- Context limits\n",
    "\n",
    "**Now let's build a RAG system to solve these problems!**\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "```\n",
    "Documents → Chunk → Embed → Store → Retrieve → Augment → Generate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: LOAD - Get Your Documents\n",
    "\n",
    "RAG starts with your knowledge base. Let's create a sample document about AI concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: No - check your .env file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "print(\"API Key loaded:\", \"Yes\" if OPENROUTER_API_KEY else \"No - check your .env file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/knowledge_base.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m\n\u001b[1;32m      4\u001b[0m knowledge_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m# TechNova Solutions - Internal Documentation\u001b[39m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Save to file\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/knowledge_base.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     86\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(knowledge_base)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated knowledge base: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(knowledge_base)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m characters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/knowledge_base.txt'"
     ]
    }
   ],
   "source": [
    "# Create a knowledge base document\n",
    "# This simulates INTERNAL company documentation that LLMs don't have access to\n",
    "\n",
    "knowledge_base = \"\"\"\n",
    "# TechNova Solutions - Internal Documentation\n",
    "\n",
    "## Company Overview\n",
    "TechNova Solutions is a Bangalore-based enterprise software company founded in 2019.\n",
    "The company specializes in cloud-native solutions and has 450 employees across 3 offices.\n",
    "Current valuation: $120 million (Series C, 2024).\n",
    "\n",
    "## Engineering Team Structure\n",
    "- Platform Team: 45 engineers, led by Rajesh Kumar\n",
    "- Backend Team: 60 engineers, led by Priya Sharma  \n",
    "- Frontend Team: 35 engineers, led by Amit Patel\n",
    "- DevOps Team: 25 engineers, led by Sneha Reddy\n",
    "- Data Engineering: 30 engineers, led by Vikram Iyer\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "### Backend Services\n",
    "- Primary Language: Go (Golang) for all microservices\n",
    "- API Framework: gRPC for internal services, REST for external APIs\n",
    "- Database: PostgreSQL 15 for transactional data, MongoDB for document storage\n",
    "- Cache: Redis Cluster with 6 nodes\n",
    "- Message Queue: Apache Kafka with 12 partitions per topic\n",
    "\n",
    "### Frontend Architecture  \n",
    "- Framework: Next.js 14 with React 18\n",
    "- State Management: Zustand (migrated from Redux in Q2 2024)\n",
    "- UI Components: Custom design system called \"Nova UI\"\n",
    "- Testing: Playwright for E2E, Vitest for unit tests\n",
    "\n",
    "### Infrastructure\n",
    "- Cloud Provider: AWS (primary), GCP (disaster recovery)\n",
    "- Kubernetes: EKS clusters in Mumbai (ap-south-1) and Singapore (ap-southeast-1)\n",
    "- Container Registry: Amazon ECR\n",
    "- CI/CD: GitHub Actions with ArgoCD for GitOps deployments\n",
    "- Monitoring: Prometheus + Grafana stack, PagerDuty for alerts\n",
    "- Logging: ELK Stack (Elasticsearch, Logstash, Kibana)\n",
    "\n",
    "### Security\n",
    "- Authentication: OAuth 2.0 with Keycloak\n",
    "- Secrets Management: HashiCorp Vault\n",
    "- WAF: AWS WAF with custom rule sets\n",
    "- Compliance: SOC 2 Type II certified, ISO 27001 in progress\n",
    "\n",
    "## Deployment Process\n",
    "1. Developer creates PR against main branch\n",
    "2. Automated tests run (unit, integration, security scans)\n",
    "3. Code review required from 2 team members\n",
    "4. Merge to main triggers staging deployment via ArgoCD\n",
    "5. QA team performs validation (2-4 hours)\n",
    "6. Production deployment requires approval from Tech Lead\n",
    "7. Canary deployment: 5% traffic for 30 minutes\n",
    "8. Full rollout if metrics are healthy\n",
    "\n",
    "## On-Call Rotation\n",
    "- Primary on-call rotates weekly across teams\n",
    "- Escalation path: On-call → Team Lead → Engineering Manager → CTO\n",
    "- SLA: P1 incidents must be acknowledged within 15 minutes\n",
    "- Post-incident reviews required for all P1/P2 incidents\n",
    "\n",
    "## Recent Incidents\n",
    "- Nov 2024: Database failover caused 23-minute outage. Root cause: misconfigured health checks.\n",
    "- Oct 2024: Kafka consumer lag spike. Resolution: increased partition count.\n",
    "- Sep 2024: Memory leak in payment service. Fixed in v2.3.4.\n",
    "\n",
    "## Q1 2025 Roadmap\n",
    "- Migrate remaining services from REST to gRPC\n",
    "- Implement distributed tracing with Jaeger\n",
    "- Launch new analytics dashboard (Project Apollo)\n",
    "- Achieve ISO 27001 certification\n",
    "- Reduce deployment time from 45 min to under 15 min\n",
    "\n",
    "## Contact Information\n",
    "- Engineering Support: eng-support@technova.internal\n",
    "- Security Team: security@technova.internal  \n",
    "- Platform Team Slack: #platform-team\n",
    "- Incident Channel: #incidents\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"../data/knowledge_base.txt\", \"w\") as f:\n",
    "    f.write(knowledge_base)\n",
    "\n",
    "print(f\"Created knowledge base: {len(knowledge_base)} characters\")\n",
    "print(\"\\nThis simulates INTERNAL company docs + recent news!\")\n",
    "print(\"\\nSections included:\")\n",
    "print(\"  - Company Overview\")\n",
    "print(\"  - Engineering Team Structure\")  \n",
    "print(\"  - Technology Stack (Backend, Frontend, Infra)\")\n",
    "print(\"  - Deployment Process\")\n",
    "print(\"  - Recent Incidents\")\n",
    "print(\"  - Q1 2025 Roadmap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Document length: 3072 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/partha/Documents/Talks/IITM_Shaastra_2026/hands_on/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"../data/knowledge_base.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Document length: {len(documents[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: CHUNK - Split Into Smaller Pieces\n",
    "\n",
    "Why chunk?\n",
    "- Embeddings work better on focused content\n",
    "- Retrieval is more precise with smaller chunks\n",
    "- Fits within LLM context limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 10 chunks\n",
      "\n",
      "==================================================\n",
      "CHUNK EXAMPLES:\n",
      "==================================================\n",
      "\n",
      "--- Chunk 1 (291 chars) ---\n",
      "# TechNova Solutions - Internal Documentation\n",
      "\n",
      "## Company Overview\n",
      "TechNova Solutions is a Bangalore-based enterprise software company founded in 2019.\n",
      "The company specializes in cloud-native solutions and has 450 employees across 3 offices.\n",
      "Current valuation: $120 million (Series C, 2024).\n",
      "\n",
      "\n",
      "--- Chunk 2 (303 chars) ---\n",
      "## Engineering Team Structure\n",
      "- Platform Team: 45 engineers, led by Rajesh Kumar\n",
      "- Backend Team: 60 engineers, led by Priya Sharma  \n",
      "- Frontend Team: 35 engineers, led by Amit Patel\n",
      "- DevOps Team: 25 engineers, led by Sneha Reddy\n",
      "- Data Engineering: 30 engineers, led by Vikram Iyer\n",
      "\n",
      "## Technology Stack\n",
      "\n",
      "\n",
      "--- Chunk 3 (337 chars) ---\n",
      "## Technology Stack\n",
      "\n",
      "### Backend Services\n",
      "- Primary Language: Go (Golang) for all microservices\n",
      "- API Framework: gRPC for internal services, REST for external APIs\n",
      "- Database: PostgreSQL 15 for transactional data, MongoDB for document storage\n",
      "- Cache: Redis Cluster with 6 nodes\n",
      "- Message Queue: Apache Kafka with 12 partitions per topic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Max characters per chunk (larger = more context per chunk)\n",
    "    chunk_overlap=100,     # Overlap to preserve context across splits\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on these first\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHUNK EXAMPLES:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: EMBED - Convert Text to Vectors\n",
    "\n",
    "Embeddings capture semantic meaning:\n",
    "- Similar concepts → similar vectors\n",
    "- Enables semantic search (not just keyword matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model (first run downloads ~90MB)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/2h77qzwj1w7g9gjz0d7yt9nh0000gn/T/ipykernel_4046/3726692373.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Using a free, local embedding model\n",
    "# First run will download the model (~90MB)\n",
    "print(\"Loading embedding model (first run downloads ~90MB)...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Fast and good quality\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "print(\"Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'What is the tech stack?'\n",
      "Embedding dimension: 384\n",
      "First 10 values: [-0.05783945694565773, -0.10159027576446533, -0.04430558905005455, -0.023304156959056854, -0.056692417711019516, -0.05945168808102608, 0.04886123538017273, 0.11376131325960159, 0.009314495138823986, -0.004337272606790066]\n",
      "\n",
      "This vector captures the MEANING of 'What is the tech stack?'\n"
     ]
    }
   ],
   "source": [
    "# Let's see what an embedding looks like\n",
    "test_text = \"What is the tech stack?\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 10 values: {test_embedding[:10]}\")\n",
    "print(f\"\\nThis vector captures the MEANING of '{test_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Embeddings: Cosine Similarity\n",
    "\n",
    "Embeddings capture **semantic meaning**. Similar sentences have similar vectors (low distance / high similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COSINE SIMILARITY BETWEEN SENTENCES\n",
      "============================================================\n",
      "\n",
      "Sentences:\n",
      "  s1: \"What is the deployment process?\"\n",
      "  s2: \"How do we deploy code to production?\"\n",
      "  s3: \"What databases does the company use?\"\n",
      "  s4: \"Tell me about the CI/CD pipeline\"\n",
      "  s5: \"What is the weather like today?\"\n",
      "\n",
      "------------------------------------------------------------\n",
      "Similarity Scores (1.0 = identical, 0.0 = unrelated):\n",
      "------------------------------------------------------------\n",
      "\n",
      "✓ s1 vs s2 (both about deployment):     0.6666  ← HIGH (similar meaning!)\n",
      "✓ s1 vs s4 (deployment vs CI/CD):       0.3548  ← MEDIUM-HIGH (related)\n",
      "\n",
      "✗ s1 vs s3 (deployment vs databases):  0.1184  ← LOWER (different topics)\n",
      "✗ s1 vs s5 (deployment vs weather):    0.0766  ← LOWEST (unrelated!)\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT: Embeddings capture MEANING, not just keywords!\n",
      "'deployment process' ≈ 'deploy code to production'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Define sentences - some similar, some different\n",
    "sentences = {\n",
    "    \"s1\": \"What is the deployment process?\",\n",
    "    \"s2\": \"How do we deploy code to production?\",      # Similar to s1\n",
    "    \"s3\": \"What databases does the company use?\",      # Different topic\n",
    "    \"s4\": \"Tell me about the CI/CD pipeline\",          # Related to s1\n",
    "    \"s5\": \"What is the weather like today?\",           # Completely unrelated\n",
    "}\n",
    "\n",
    "# Get embeddings for all sentences\n",
    "sentence_embeddings = {key: embeddings.embed_query(text) for key, text in sentences.items()}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COSINE SIMILARITY BETWEEN SENTENCES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSentences:\")\n",
    "for key, text in sentences.items():\n",
    "    print(f\"  {key}: \\\"{text}\\\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Similarity Scores (1.0 = identical, 0.0 = unrelated):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Compare similar sentences\n",
    "sim_1_2 = cosine_similarity(sentence_embeddings[\"s1\"], sentence_embeddings[\"s2\"])\n",
    "print(f\"\\n✓ s1 vs s2 (both about deployment):     {sim_1_2:.4f}  ← HIGH (similar meaning!)\")\n",
    "\n",
    "sim_1_4 = cosine_similarity(sentence_embeddings[\"s1\"], sentence_embeddings[\"s4\"])\n",
    "print(f\"✓ s1 vs s4 (deployment vs CI/CD):       {sim_1_4:.4f}  ← MEDIUM-HIGH (related)\")\n",
    "\n",
    "# Compare different sentences\n",
    "sim_1_3 = cosine_similarity(sentence_embeddings[\"s1\"], sentence_embeddings[\"s3\"])\n",
    "print(f\"\\n✗ s1 vs s3 (deployment vs databases):  {sim_1_3:.4f}  ← LOWER (different topics)\")\n",
    "\n",
    "sim_1_5 = cosine_similarity(sentence_embeddings[\"s1\"], sentence_embeddings[\"s5\"])\n",
    "print(f\"✗ s1 vs s5 (deployment vs weather):    {sim_1_5:.4f}  ← LOWEST (unrelated!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT: Embeddings capture MEANING, not just keywords!\")\n",
    "print(\"'deployment process' ≈ 'deploy code to production'\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: STORE - Save in Vector Database\n",
    "\n",
    "Vector databases enable fast similarity search across millions of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store (embedding all chunks)...\n",
      "\n",
      "Vector store created!\n",
      "Contains 25 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create vector store from chunks\n",
    "# This embeds all chunks and stores them\n",
    "print(\"Creating vector store (embedding all chunks)...\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"../data/chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"\\nVector store created!\")\n",
    "print(f\"Contains {vectorstore._collection.count()} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: RETRIEVE - Find Relevant Chunks\n",
    "\n",
    "Given a query, find the most similar chunks using vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What databases does TechNova use?'\n",
      "\n",
      "Retrieved 4 relevant chunks:\n",
      "==================================================\n",
      "\n",
      "--- Match 1 ---\n",
      "# TechNova Solutions - Internal Documentation\n",
      "\n",
      "## Company Overview\n",
      "TechNova Solutions is a Bangalore-based enterprise software company founded in 2019.\n",
      "The company specializes in cloud-native solutions and has 450 employees across 3 offices.\n",
      "Current valuation: $120 million (Series C, 2024).\n",
      "\n",
      "--- Match 2 ---\n",
      "# TechNova Solutions - Internal Documentation\n",
      "\n",
      "## Company Overview\n",
      "TechNova Solutions is a Bangalore-based enterprise software company founded in 2019.\n",
      "The company specializes in cloud-native solutions and has 450 employees across 3 offices.\n",
      "Current valuation: $120 million (Series C, 2024).\n",
      "\n",
      "--- Match 3 ---\n",
      "## Contact Information\n",
      "- Engineering Support: eng-support@technova.internal\n",
      "- Security Team: security@technova.internal  \n",
      "- Platform Team Slack: #platform-team\n",
      "- Incident Channel: #incidents\n",
      "\n",
      "--- Match 4 ---\n",
      "## Q1 2025 Roadmap\n",
      "- Migrate remaining services from REST to gRPC\n",
      "- Implement distributed tracing with Jaeger\n",
      "- Launch new analytics dashboard (Project Apollo)\n",
      "- Achieve ISO 27001 certification\n",
      "- Reduce deployment time from 45 min to under 15 min\n",
      "\n",
      "## Contact Information\n",
      "- Engineering Support: eng-support@technova.internal\n",
      "- Security Team: security@technova.internal  \n",
      "- Platform Team Slack: #platform-team\n",
      "- Incident Channel: #incidents\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 matches for better coverage\n",
    ")\n",
    "\n",
    "# Test retrieval with company data\n",
    "query = \"What databases does TechNova use?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} relevant chunks:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Match {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: AUGMENT & GENERATE - Build the RAG Chain\n",
    "\n",
    "Now we combine retrieval with the LLM to generate grounded answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"anthropic/claude-3.5-sonnet\",\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=OPENROUTER_BASE_URL,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# RAG Prompt Template\n",
    "template = \"\"\"You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "If the context doesn't contain the answer, say \"I don't have information about that in my knowledge base.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Let's Test It!\n",
    "\n",
    "### Test 1: Questions about our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: In which programming language is TechNova's backend services written?\n",
      "==================================================\n",
      "A: According to the context, TechNova's backend services are written in Go (Golang) for all microservices.\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Internal company data (LLM has NO access to this!)\n",
    "question = \"In which programming language is TechNova's backend services written?\"\n",
    "\n",
    "print(f\"Q: {question}\")\n",
    "print(\"=\"*50)\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the deployment process at TechNova? Who needs to approve production deployments?\n",
      "==================================================\n",
      "A: Based on the context, the deployment process at TechNova consists of these steps:\n",
      "\n",
      "1. Developer creates PR against main branch\n",
      "2. Automated tests run (unit, integration, security scans)\n",
      "3. Code review required from 2 team members\n",
      "4. Merge to main triggers staging deployment via ArgoCD\n",
      "5. QA team performs validation (2-4 hours)\n",
      "6. Production deployment requires approval from Tech Lead\n",
      "7. Canary deployment: 5% traffic for 30 minutes\n",
      "8. Full rollout if metrics are healthy\n",
      "\n",
      "Specifically regarding approvals, the Tech Lead needs to approve production deployments.\n"
     ]
    }
   ],
   "source": [
    "# Question 3: More internal data\n",
    "question = \"What is the deployment process at TechNova? Who needs to approve production deployments?\"\n",
    "\n",
    "print(f\"Q: {question}\")\n",
    "print(\"=\"*50)\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Question NOT in knowledge base\n",
    "\n",
    "RAG should gracefully handle questions outside its knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who won the 2024 Nobel Prize in Physics?\n",
      "==================================================\n",
      "A: I don't have information about that in my knowledge base. The provided context only contains information about engineering team structure, frontend architecture, and technology stack. It does not contain any information about Nobel Prize winners.\n",
      "\n",
      "(RAG correctly says it doesn't have this information!)\n"
     ]
    }
   ],
   "source": [
    "# Question not in our knowledge base\n",
    "question = \"Who won the 2024 Nobel Prize in Physics?\"\n",
    "\n",
    "print(f\"Q: {question}\")\n",
    "print(\"=\"*50)\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"A: {answer}\")\n",
    "print(\"\\n(RAG correctly says it doesn't have this information!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Magic: Compare With vs Without RAG\n",
    "\n",
    "Let's see the difference RAG makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO: Private Company Data - With vs Without RAG\n",
      "======================================================================\n",
      "\n",
      "Question: Whats is the team size and lead name for the dev ops team at TechNova?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT RAG (LLM alone):\n",
      "----------------------------------------------------------------------\n",
      "I cannot provide specific information about TechNova's DevOps team size or lead name, as I don't have access to their internal organizational details. To get accurate information about TechNova's team structure, you would need to contact TechNova directly or consult their official company resources.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "WITH RAG (LLM + TechNova knowledge base):\n",
      "----------------------------------------------------------------------\n",
      "According to the context, the DevOps Team has 25 engineers and is led by Sneha Reddy.\n",
      "\n",
      "======================================================================\n",
      "KEY TAKEAWAY\n",
      "======================================================================\n",
      "Without RAG: LLM has no access to private company data\n",
      "With RAG: Accurate answers from YOUR internal knowledge base!\n",
      "\n",
      "This is why RAG is essential for enterprise applications.\n"
     ]
    }
   ],
   "source": [
    "# Direct LLM (no RAG) - for comparison\n",
    "def ask_without_rag(question):\n",
    "    response = llm.invoke(question)\n",
    "    return response.content\n",
    "\n",
    "# THE KEY DEMO: Private company data that LLM cannot know!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO: Private Company Data - With vs Without RAG\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "question = \"Whats is the team size and lead name for the dev ops team at TechNova?\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"WITHOUT RAG (LLM alone):\")\n",
    "print(\"-\" * 70)\n",
    "print(ask_without_rag(question))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"WITH RAG (LLM + TechNova knowledge base):\")\n",
    "print(\"-\" * 70)\n",
    "print(rag_chain.invoke(question))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Without RAG: LLM has no access to private company data\")\n",
    "print(\"With RAG: Accurate answers from YOUR internal knowledge base!\")\n",
    "print(\"\\nThis is why RAG is essential for enterprise applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Built\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│   LOAD      │ →   │   CHUNK     │ →   │   EMBED     │ →   │   STORE     │\n",
    "│  Documents  │     │  Split text │     │  Vectors    │     │  ChromaDB   │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘\n",
    "                                                                   ↓\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│   ANSWER    │ ←   │  GENERATE   │ ←   │  AUGMENT    │ ←   │  RETRIEVE   │\n",
    "│  Grounded!  │     │    LLM      │     │  Prompt     │     │  Similar    │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘\n",
    "```\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Embeddings** capture semantic meaning - similar sentences have high cosine similarity\n",
    "2. **Chunking** strategy matters - too small loses context, too large loses precision\n",
    "3. **RAG grounds LLMs** in your data - no more hallucinations about your content\n",
    "4. **Private data stays private** - only retrieved context is sent to the LLM\n",
    "\n",
    "### Next: Agentic RAG\n",
    "Basic RAG has limitations. What if:\n",
    "- The first retrieval doesn't find good results?\n",
    "- The question needs to be rewritten?\n",
    "- Multiple lookups are needed?\n",
    "\n",
    "**Agentic RAG adds intelligence to decide WHEN and HOW to retrieve!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
